{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets accelerate evaluate peft rapidfuzz --quiet\n!pip install --upgrade transformers peft accelerate bitsandbytes --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T07:24:03.741525Z","iopub.execute_input":"2025-05-04T07:24:03.741764Z","iopub.status.idle":"2025-05-04T07:25:35.520209Z","shell.execute_reply.started":"2025-05-04T07:24:03.741739Z","shell.execute_reply":"2025-05-04T07:25:35.519200Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM ,T5Tokenizer, T5ForConditionalGeneration\nfrom peft import PeftModel\nfrom datasets import load_dataset\nfrom fuzzywuzzy import fuzz\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T07:25:35.522297Z","iopub.execute_input":"2025-05-04T07:25:35.522557Z","iopub.status.idle":"2025-05-04T07:25:57.402035Z","shell.execute_reply.started":"2025-05-04T07:25:35.522536Z","shell.execute_reply":"2025-05-04T07:25:57.401200Z"}},"outputs":[{"name":"stderr","text":"2025-05-04 07:25:45.869365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746343546.051497      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746343546.104060      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"model_name = 'google/flan-t5-large'\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T07:25:57.402880Z","iopub.execute_input":"2025-05-04T07:25:57.403736Z","iopub.status.idle":"2025-05-04T07:26:31.800059Z","shell.execute_reply.started":"2025-05-04T07:25:57.403705Z","shell.execute_reply":"2025-05-04T07:26:31.799251Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45c23c46dbd44801b18d6c6da6b643f1"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bb9b1641440448db11a6023849fd1b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd3a95e1583240cd90720e21c7babb88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efbec65934dd4102b863b7cc6d7bb1e8"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80c22fd00c9b48478188d21fddf75694"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d64501b6fce24ad6aceead53d2160288"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"723f704bda3946e280655b778b10abb3"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the first 1000 examples from the training split\nsquad = load_dataset(\"squad\", split=\"train[:1000]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T07:26:31.800888Z","iopub.execute_input":"2025-05-04T07:26:31.801083Z","iopub.status.idle":"2025-05-04T07:26:39.254466Z","shell.execute_reply.started":"2025-05-04T07:26:31.801065Z","shell.execute_reply":"2025-05-04T07:26:39.253970Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02f521b5f0b94ef19c438289065c0f6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a4762bfe5ff4fb4b77de7e8d8b686c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41643353fe6d407bab745282c035ce65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cf5794ea96e4e13b976183bda4c9852"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ee6b5dad0864a248abcda028a797b0b"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"squad","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T07:26:39.255137Z","iopub.execute_input":"2025-05-04T07:26:39.255401Z","iopub.status.idle":"2025-05-04T07:26:39.260069Z","shell.execute_reply.started":"2025-05-04T07:26:39.255371Z","shell.execute_reply":"2025-05-04T07:26:39.259407Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'title', 'context', 'question', 'answers'],\n    num_rows: 1000\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_data = [\n    {\n        \"context\": item[\"context\"],\n        \"question\": item[\"question\"],\n        \"answers\": item[\"answers\"]\n    }\n    for item in squad\n]\ntrain_dataset = Dataset.from_list(train_data)\ntrain_dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T07:26:39.261977Z","iopub.execute_input":"2025-05-04T07:26:39.262520Z","iopub.status.idle":"2025-05-04T07:26:40.365717Z","shell.execute_reply.started":"2025-05-04T07:26:39.262503Z","shell.execute_reply":"2025-05-04T07:26:40.364992Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['context', 'question', 'answers'],\n    num_rows: 1000\n})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ntotal_score = 0\ncount = 0\n\nfor example in squad:\n    context = example[\"context\"]\n    question = example[\"question\"]\n    reference = example[\"answers\"][\"text\"][0]\n\n    input_text = f\"question: {question} context: {context}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(device)\n\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_length=64)\n        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    score = fuzz.token_sort_ratio(prediction, reference)\n    total_score += score\n    count += 1\n\naverage_score = total_score / count\nprint(f\"\\nğŸ” Average Fuzzy Score: {average_score:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T07:26:40.366581Z","iopub.execute_input":"2025-05-04T07:26:40.366826Z","iopub.status.idle":"2025-05-04T07:29:45.803989Z","shell.execute_reply.started":"2025-05-04T07:26:40.366807Z","shell.execute_reply":"2025-05-04T07:29:45.803285Z"}},"outputs":[{"name":"stdout","text":"\nğŸ” Average Fuzzy Score: 92.70\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# With Lora ","metadata":{}},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q\", \"v\"],  # Adapted for T5\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T07:29:45.804834Z","iopub.execute_input":"2025-05-04T07:29:45.805232Z","iopub.status.idle":"2025-05-04T07:29:46.002698Z","shell.execute_reply.started":"2025-05-04T07:29:45.805195Z","shell.execute_reply":"2025-05-04T07:29:46.002099Z"}},"outputs":[{"name":"stdout","text":"trainable params: 2,359,296 || all params: 785,509,376 || trainable%: 0.3004\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def preprocess(example):\n    return {\n        \"input\": f\"Context: {example['context']}\\nQuestion: {example['question']}\",\n        \"output\": example[\"answers\"][\"text\"][0] if example[\"answers\"][\"text\"] else \"\"\n    }\n\nformatted_dataset = squad.map(preprocess)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T07:29:46.003528Z","iopub.execute_input":"2025-05-04T07:29:46.003795Z","iopub.status.idle":"2025-05-04T07:29:46.131177Z","shell.execute_reply.started":"2025-05-04T07:29:46.003769Z","shell.execute_reply":"2025-05-04T07:29:46.130643Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af8a8145dcbe4719a89b513300c7e466"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"def tokenize(batch):\n    inputs = tokenizer(batch[\"input\"], padding=\"max_length\", truncation=True, max_length=512)\n    targets = tokenizer(batch[\"output\"], padding=\"max_length\", truncation=True, max_length=128)\n\n    inputs[\"labels\"] = targets[\"input_ids\"]\n    return inputs\n\ntokenized_dataset = formatted_dataset.map(tokenize, batched=True, remove_columns=[\"input\", \"output\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T07:29:46.131983Z","iopub.execute_input":"2025-05-04T07:29:46.132645Z","iopub.status.idle":"2025-05-04T07:29:47.136233Z","shell.execute_reply.started":"2025-05-04T07:29:46.132626Z","shell.execute_reply":"2025-05-04T07:29:47.135657Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28c37e9a4e134c60b8b82381f8552e59"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForSeq2Seq\n)\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"lora-output\",\n    per_device_train_batch_size=1,\n    num_train_epochs=3,\n    learning_rate=2e-4,\n    logging_dir=\"logs\",\n    save_strategy=\"no\",\n    fp16=False,\n    report_to=\"none\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T07:29:47.136974Z","iopub.execute_input":"2025-05-04T07:29:47.137212Z","iopub.status.idle":"2025-05-04T07:44:34.112855Z","shell.execute_reply.started":"2025-05-04T07:29:47.137186Z","shell.execute_reply":"2025-05-04T07:44:34.112246Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2741126461.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\nNo label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3000/3000 14:44, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>12.601300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.047900</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.011900</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.012300</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.008900</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.008300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3000, training_loss=2.115082941691081, metrics={'train_runtime': 885.439, 'train_samples_per_second': 3.388, 'train_steps_per_second': 3.388, 'total_flos': 6936047124480000.0, 'train_loss': 2.115082941691081, 'epoch': 3.0})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"output_dir = \"./output\"\ntrainer.save_model(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T07:44:41.517487Z","iopub.status.idle":"2025-05-04T07:44:41.517743Z","shell.execute_reply.started":"2025-05-04T07:44:41.517632Z","shell.execute_reply":"2025-05-04T07:44:41.517647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model_name = \"google/flan-t5-large\"\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\nmodel = PeftModel.from_pretrained(base_model, \"./output\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T07:44:49.611246Z","iopub.execute_input":"2025-05-04T07:44:49.611763Z","iopub.status.idle":"2025-05-04T07:44:51.263906Z","shell.execute_reply.started":"2025-05-04T07:44:49.611737Z","shell.execute_reply":"2025-05-04T07:44:51.263349Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"squad = load_dataset(\"squad\", split=\"validation[:10]\")\n\ntest_data = []\nfor ex in squad:\n    input_text = f\"Context: {ex['context']}\\nQuestion: {ex['question']}\"\n    ground_truth = ex[\"answers\"][\"text\"][0] if ex[\"answers\"][\"text\"] else \"\"\n    test_data.append((input_text, ground_truth))\nsquad.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T07:51:22.008998Z","iopub.execute_input":"2025-05-04T07:51:22.009711Z","iopub.status.idle":"2025-05-04T07:51:26.784438Z","shell.execute_reply.started":"2025-05-04T07:51:22.009687Z","shell.execute_reply":"2025-05-04T07:51:26.783715Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"(10, 5)"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"def get_prediction(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n    output_ids = model.generate(**inputs, max_length=50)\n    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\ntotal_score = 0\n\nfor input_text, ground_truth in test_data:\n    prediction = get_prediction(input_text)\n    score = fuzz.token_sort_ratio(prediction, ground_truth)\n    total_score += score\n\navg_score = total_score / len(test_data)\nprint(f\"\\nâœ… Average Fuzzy Match Score: {avg_score:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T07:51:28.789018Z","iopub.execute_input":"2025-05-04T07:51:28.789291Z","iopub.status.idle":"2025-05-04T07:51:41.367259Z","shell.execute_reply.started":"2025-05-04T07:51:28.789270Z","shell.execute_reply":"2025-05-04T07:51:41.366627Z"}},"outputs":[{"name":"stdout","text":"\nâœ… Average Fuzzy Match Score: 96.30%\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# With QLora","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import AutoModelForSeq2SeqLM,BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    \"google/flan-t5-large\",\n    quantization_config=bnb_config,\n)\nmodel = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q\", \"v\", \"wi\", \"wo\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"SEQ_2_SEQ_LM\"\n)\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:04:32.238872Z","iopub.execute_input":"2025-05-04T08:04:32.239612Z","iopub.status.idle":"2025-05-04T08:04:35.162283Z","shell.execute_reply.started":"2025-05-04T08:04:32.239589Z","shell.execute_reply":"2025-05-04T08:04:35.161721Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForSeq2Seq\n)\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./qlora-output\",\n    per_device_train_batch_size=1,\n    num_train_epochs=2,\n    learning_rate=2e-4,\n    logging_dir=\"logs\",\n    save_strategy=\"no\",\n    fp16=False,\n    report_to=\"none\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)\n)\n\n# Train model\ntrainer.train()\n# trainer.save_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:04:40.493270Z","iopub.execute_input":"2025-05-04T08:04:40.494004Z","iopub.status.idle":"2025-05-04T08:27:53.997857Z","shell.execute_reply.started":"2025-05-04T08:04:40.493980Z","shell.execute_reply":"2025-05-04T08:27:53.997252Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/931716462.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\nNo label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 23:12, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2000, training_loss=0.0, metrics={'train_runtime': 1392.9554, 'train_samples_per_second': 1.436, 'train_steps_per_second': 1.436, 'total_flos': 4633091112960000.0, 'train_loss': 0.0, 'epoch': 2.0})"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"output_dir = \"./qlora-output\"\ntrainer.save_model(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:28:37.554880Z","iopub.execute_input":"2025-05-04T08:28:37.555673Z","iopub.status.idle":"2025-05-04T08:29:01.309942Z","shell.execute_reply.started":"2025-05-04T08:28:37.555647Z","shell.execute_reply":"2025-05-04T08:29:01.309255Z"}},"outputs":[{"name":"stdout","text":"\nâœ… Average Fuzzy Match Score: 96.30%\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"base_model_name = \"google/flan-t5-large\"\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\nmodel = PeftModel.from_pretrained(base_model, \"./qlora-output\")\nsquad = load_dataset(\"squad\", split=\"validation[:1000]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:32:48.183791Z","iopub.execute_input":"2025-05-04T08:32:48.184276Z","iopub.status.idle":"2025-05-04T08:32:55.325796Z","shell.execute_reply.started":"2025-05-04T08:32:48.184251Z","shell.execute_reply":"2025-05-04T08:32:55.325069Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"test_data = []\nfor ex in squad:\n    input_text = f\"Context: {ex['context']}\\nQuestion: {ex['question']}\"\n    ground_truth = ex[\"answers\"][\"text\"][0] if ex[\"answers\"][\"text\"] else \"\"\n    test_data.append((input_text, ground_truth))\n\ndef get_prediction(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n    output_ids = model.generate(**inputs, max_length=50)\n    return tokenizer.decode(output_ids[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:32:55.326982Z","iopub.execute_input":"2025-05-04T08:32:55.327234Z","iopub.status.idle":"2025-05-04T08:32:55.401174Z","shell.execute_reply.started":"2025-05-04T08:32:55.327207Z","shell.execute_reply":"2025-05-04T08:32:55.400351Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"total_score = 0\n\nfor input_text, ground_truth in test_data:\n    prediction = get_prediction(input_text)\n    score = fuzz.token_sort_ratio(prediction, ground_truth)\n    total_score += score\n\navg_score = total_score / len(test_data)\nprint(f\"\\nâœ… Average Fuzzy Match Score: {avg_score:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:33:05.181242Z","iopub.execute_input":"2025-05-04T08:33:05.181809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}